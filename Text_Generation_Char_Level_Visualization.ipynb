{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Set the seed for random operations. \n",
    "# This let our experiments to be reproducible. \n",
    "SEED = 12\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Get current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Set GPU memory growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generation - Next character prediction\n",
    "## Charles Dickens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full text length: 157789\n",
      "Number of unique characters: 65\n",
      "['\\n', ' ', '!', '\"', \"'\", '(', ')', ',', '-', '.', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'ö']\n"
     ]
    }
   ],
   "source": [
    "# Prepare dataset\n",
    "# ---------------\n",
    "\n",
    "# Read full text\n",
    "with open(os.path.join(cwd, 'dickens.txt'), 'r') as f:\n",
    "    full_text = f.read()\n",
    "f.close()\n",
    "\n",
    "full_text_length = len(full_text)\n",
    "print('Full text length:', full_text_length)\n",
    "\n",
    "# Create vocabulary\n",
    "vocabulary = sorted(list(set(full_text)))\n",
    "\n",
    "print('Number of unique characters:', len(vocabulary))\n",
    "print(vocabulary)\n",
    "\n",
    "# Dictionaries for char-to-int/int-to-char conversion\n",
    "ctoi = {c:i for i, c in enumerate(vocabulary)}\n",
    "itoc = {i:c for i, c in enumerate(vocabulary)}\n",
    "\n",
    "seq_length = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Recurrent Neural Network\n",
    "# ------------------------------\n",
    "\n",
    "# Hidden size (state)\n",
    "h_size = 128\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.LSTM(units=h_size, batch_input_shape=[None, seq_length, len(vocabulary)], \n",
    "                               return_sequences=True, stateful=False))\n",
    "model.add(tf.keras.layers.LSTM(units=h_size, return_sequences=False, stateful=False))\n",
    "model.add(tf.keras.layers.Dense(units=len(vocabulary), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 100, 128)          99328     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 65)                8385      \n",
      "=================================================================\n",
      "Total params: 239,297\n",
      "Trainable params: 239,297\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'lstm/kernel:0' shape=(65, 512) dtype=float32, numpy=\n",
       " array([[-0.00464945, -0.03394137, -0.000799  , ..., -0.02225107,\n",
       "         -0.06204449,  0.03887913],\n",
       "        [-0.07563964,  0.09494284, -0.01195837, ...,  0.06521125,\n",
       "         -0.06513311, -0.06987695],\n",
       "        [-0.00498689,  0.05380745,  0.04336335, ..., -0.01257289,\n",
       "          0.09079292, -0.02371988],\n",
       "        ...,\n",
       "        [ 0.00502372, -0.04001955, -0.01122119, ...,  0.09009796,\n",
       "         -0.07304128, -0.07973754],\n",
       "        [-0.00993448, -0.08406316, -0.08523064, ...,  0.0843312 ,\n",
       "         -0.04517664, -0.05949436],\n",
       "        [ 0.09082402, -0.08354253, -0.06263713, ...,  0.05646159,\n",
       "         -0.06945679, -0.04800734]], dtype=float32)>,\n",
       " <tf.Variable 'lstm/recurrent_kernel:0' shape=(128, 512) dtype=float32, numpy=\n",
       " array([[-6.6588640e-02,  3.6324739e-02, -8.2312122e-02, ...,\n",
       "          2.5029799e-02, -1.3928058e-02, -7.1377113e-02],\n",
       "        [-1.1753984e-02,  2.6283264e-02, -2.6581895e-02, ...,\n",
       "         -4.0398641e-03, -3.0838365e-02, -3.6438081e-02],\n",
       "        [ 3.8994774e-03, -5.0685365e-02,  1.6511202e-02, ...,\n",
       "          4.2907265e-03, -4.7752131e-03,  9.7082695e-05],\n",
       "        ...,\n",
       "        [ 1.2579722e-02, -2.3764292e-02, -1.6915673e-02, ...,\n",
       "          2.9805612e-02,  3.9233245e-02,  5.0380178e-02],\n",
       "        [-1.6430324e-02, -4.6492619e-03,  4.3443713e-02, ...,\n",
       "          5.1529538e-03,  1.8974684e-02, -1.0254180e-02],\n",
       "        [-1.2456688e-02,  1.1361114e-02,  1.5396808e-02, ...,\n",
       "         -5.4451656e-03,  5.3713173e-03, -6.9648251e-02]], dtype=float32)>,\n",
       " <tf.Variable 'lstm/bias:0' shape=(512,) dtype=float32, numpy=\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0.], dtype=float32)>,\n",
       " <tf.Variable 'lstm_1/kernel:0' shape=(128, 512) dtype=float32, numpy=\n",
       " array([[-0.02211628, -0.00715739, -0.06360492, ...,  0.03146011,\n",
       "         -0.04858107,  0.03706476],\n",
       "        [ 0.04012297, -0.02060257, -0.05741914, ...,  0.08451943,\n",
       "          0.02295278, -0.03245457],\n",
       "        [-0.0776675 , -0.01580857, -0.01676426, ...,  0.03734286,\n",
       "         -0.0481289 ,  0.02736267],\n",
       "        ...,\n",
       "        [ 0.01601264, -0.02639008,  0.03859092, ..., -0.03191078,\n",
       "         -0.05928218,  0.01108753],\n",
       "        [-0.03083446,  0.05860385,  0.05369651, ...,  0.00441309,\n",
       "         -0.03577319,  0.00271699],\n",
       "        [-0.0891901 ,  0.07860358, -0.06389736, ...,  0.04421107,\n",
       "         -0.00674027,  0.03277926]], dtype=float32)>,\n",
       " <tf.Variable 'lstm_1/recurrent_kernel:0' shape=(128, 512) dtype=float32, numpy=\n",
       " array([[-0.03562593, -0.04601286,  0.01320568, ..., -0.00029318,\n",
       "         -0.00947573,  0.0179547 ],\n",
       "        [ 0.03298601,  0.05539691, -0.00389026, ..., -0.04413474,\n",
       "          0.01828435, -0.01486933],\n",
       "        [-0.0269292 , -0.02859456,  0.05362713, ...,  0.04347036,\n",
       "         -0.0392465 , -0.03867769],\n",
       "        ...,\n",
       "        [ 0.01161965, -0.02853455, -0.02209279, ...,  0.04357574,\n",
       "          0.05578917, -0.01309427],\n",
       "        [ 0.04123621,  0.06534674,  0.02455151, ...,  0.00580133,\n",
       "          0.03650715, -0.10648234],\n",
       "        [-0.14567457, -0.14935632,  0.03275269, ...,  0.01530004,\n",
       "          0.02720911, -0.01305345]], dtype=float32)>,\n",
       " <tf.Variable 'lstm_1/bias:0' shape=(512,) dtype=float32, numpy=\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0.], dtype=float32)>,\n",
       " <tf.Variable 'dense/kernel:0' shape=(128, 65) dtype=float32, numpy=\n",
       " array([[ 0.10571329,  0.1392292 , -0.03588873, ...,  0.09454031,\n",
       "          0.03306878,  0.07945319],\n",
       "        [ 0.15374263, -0.03051007,  0.1448446 , ...,  0.15938692,\n",
       "         -0.04119605,  0.08354814],\n",
       "        [-0.16266178,  0.10004054, -0.13981527, ...,  0.07913955,\n",
       "          0.13314287, -0.00520833],\n",
       "        ...,\n",
       "        [-0.13825   ,  0.0586399 , -0.02598019, ...,  0.01472817,\n",
       "         -0.04821666,  0.07362328],\n",
       "        [ 0.07095908, -0.05840747, -0.11840581, ..., -0.14455655,\n",
       "          0.03053142, -0.00405864],\n",
       "        [ 0.06090845, -0.00448503, -0.143757  , ..., -0.14939949,\n",
       "         -0.10230137, -0.12369493]], dtype=float32)>,\n",
       " <tf.Variable 'dense/bias:0' shape=(65,) dtype=float32, numpy=\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       dtype=float32)>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summary()\n",
    "model.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization params\n",
    "# -------------------\n",
    "\n",
    "# Loss\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "# learning rate\n",
    "lr = 1e-2\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "# -------------------\n",
    "\n",
    "# Validation metrics\n",
    "# ------------------\n",
    "\n",
    "metrics = ['accuracy']\n",
    "# ------------------\n",
    "\n",
    "# Compile Model\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Seed sequence:\n",
      " cried.\n",
      "\n",
      "Shaving was not an easy task, for his hand continued to\n",
      "shake very much; and shaving requir\n",
      "\n",
      "----- Generated Sentence\n",
      " cried.\n",
      "\n",
      "Shaving was not an easy task, for his hand continued to\n",
      "shake very much; and shaving requirS,klDQ:zrCQ)mzp!UgUNoRk'MtBtZv'jNq!YDz?jSvrJ,lACUwPF-m,GINTxBooSöQBUyRWöVprIwncQ:CQDHx\"t?iqu-puQ .U'\n",
      "\n",
      "----- Original Sentence\n",
      " cried.\n",
      "\n",
      "Shaving was not an easy task, for his hand continued to\n",
      "shake very much; and shaving requires attention, even when\n",
      "you don't dance while you are at it. But if he had cut the\n",
      "end of his nose o\n"
     ]
    }
   ],
   "source": [
    "# Set number of characters we want to generate\n",
    "generation_length = 100\n",
    "\n",
    "# Get random seed sequence\n",
    "start_idx = np.random.randint(0, full_text_length - seq_length)\n",
    "\n",
    "seed_sentence = full_text[start_idx:start_idx+seq_length]\n",
    "\n",
    "print('----- Seed sequence:')\n",
    "print(seed_sentence)\n",
    "\n",
    "in_onehot = np.zeros([1, seq_length, len(vocabulary)])\n",
    "for t_idx, c in enumerate(seed_sentence):\n",
    "    in_onehot[:, t_idx, ctoi[c]] = 1.\n",
    "    \n",
    "generated_sentence = seed_sentence\n",
    "    \n",
    "for i in range(generation_length):\n",
    "        \n",
    "    preds = model.predict(in_onehot, verbose=0)[0]\n",
    "    \n",
    "    # Two main ways of predicting\n",
    "    # dummy: argmax\n",
    "    # next_char = np.argmax(preds[-1], temperature=0.5)\n",
    "    # sampling\n",
    "    # less the temperature more predictable is the output\n",
    "    next_char = sample(preds, temperature=0.5)  # next_char is the id\n",
    "    \n",
    "    next_char_onehot = np.zeros([1, 1, len(vocabulary)])\n",
    "    next_char_onehot[:, :, next_char] = 1.\n",
    "    \n",
    "    in_onehot = np.concatenate([in_onehot, next_char_onehot], axis=1)\n",
    "    in_onehot = in_onehot[:, 1:, :]\n",
    "    \n",
    "    generated_sentence += itoc[next_char]\n",
    "\n",
    "print('\\n----- Generated Sentence')\n",
    "print(generated_sentence)\n",
    "\n",
    "print('\\n----- Original Sentence')\n",
    "original_sentence = full_text[start_idx:start_idx+len(generated_sentence)]\n",
    "print(original_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model epoch: 1\n",
      "###############\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Unsuccessful TensorSliceReader constructor: Failed to get matching files on /Users/gleonardo/Desktop/DeepLearning/Lesson3/Notebooks/dickens_experiments/exp_Dec12_22-18-56/ckpts/cp_01.ckpt: Not found: /Users/gleonardo/Desktop/DeepLearning/Lesson3/Notebooks/dickens_experiments/exp_Dec12_22-18-56/ckpts; No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-21aed5e7e48d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Load Model at current epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     model.load_weights(os.path.join(\n\u001b[0;32m---> 15\u001b[0;31m         cwd, 'dickens_experiments', 'exp_Dec12_22-18-56', 'ckpts', 'cp_'+epoch_str+'.ckpt'))\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n----- Seed sequence:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name)\u001b[0m\n\u001b[1;32m    179\u001b[0m         raise ValueError('Load weights is not yet supported with TPUStrategy '\n\u001b[1;32m    180\u001b[0m                          'with steps_per_run greater than 1.')\n\u001b[0;32m--> 181\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mtrackable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_automatic_dependency_tracking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name)\u001b[0m\n\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1143\u001b[0;31m         \u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNewCheckpointReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1144\u001b[0m         \u001b[0msave_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'tf'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0merrors_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLossError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mNewCheckpointReader\u001b[0;34m(filepattern)\u001b[0m\n\u001b[1;32m    871\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mNewCheckpointReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mCheckpointReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[0mNewCheckpointReader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tf_api_names_v1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'train.NewCheckpointReader'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mthis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_pywrap_tensorflow_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_CheckpointReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to get matching files on /Users/gleonardo/Desktop/DeepLearning/Lesson3/Notebooks/dickens_experiments/exp_Dec12_22-18-56/ckpts/cp_01.ckpt: Not found: /Users/gleonardo/Desktop/DeepLearning/Lesson3/Notebooks/dickens_experiments/exp_Dec12_22-18-56/ckpts; No such file or directory"
     ]
    }
   ],
   "source": [
    "# This code is used to visualize different results at different epochs (starting, intermediate, final)\n",
    "# ADAPT THIS TO YOUR CODE\n",
    "for epoch in range(1, 101, 49):\n",
    "    \n",
    "    print('\\nModel epoch:', epoch)\n",
    "    print('###############')\n",
    "    \n",
    "    if epoch < 10:\n",
    "        epoch_str = '0'+str(epoch)\n",
    "    else:\n",
    "        epoch_str = str(epoch)\n",
    "    \n",
    "    # Load Model at current epoch\n",
    "    model.load_weights(os.path.join(\n",
    "        cwd, 'dickens_experiments', 'exp_Dec12_22-18-56', 'ckpts', 'cp_'+epoch_str+'.ckpt'))\n",
    "    \n",
    "    print('\\n----- Seed sequence:')\n",
    "    print(seed_sentence)\n",
    "\n",
    "    in_onehot = np.zeros([1, seq_length, len(vocabulary)])\n",
    "    for t_idx, c in enumerate(seed_sentence):\n",
    "        in_onehot[:, t_idx, ctoi[c]] = 1.\n",
    "\n",
    "    generated_sentence = seed_sentence\n",
    "\n",
    "    for i in range(generation_length):\n",
    "\n",
    "        preds = model.predict(in_onehot, verbose=0)[0]\n",
    "\n",
    "        # Two main ways of predicting\n",
    "        # dummy: argmax\n",
    "        # next_char = np.argmax(preds[-1])\n",
    "        # sampling\n",
    "        # less the temperature more predictable is the output\n",
    "        next_char = sample(preds, temperature=0.5)  # next_char is the id\n",
    "\n",
    "        next_char_onehot = np.zeros([1, 1, len(vocabulary)])\n",
    "        next_char_onehot[:, :, next_char] = 1.\n",
    "\n",
    "        in_onehot = np.concatenate([in_onehot, next_char_onehot], axis=1)\n",
    "        in_onehot = in_onehot[:, 1:, :]\n",
    "\n",
    "        generated_sentence += itoc[next_char]\n",
    "\n",
    "\n",
    "    print('\\n----- Generated Sentence')\n",
    "    print(generated_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize most probable future characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Unsuccessful TensorSliceReader constructor: Failed to get matching files on /Users/gleonardo/Desktop/DeepLearning/Lesson3/Notebooks/dickens_experiments/exp_Dec12_22-18-56/ckpts/cp_100.ckpt: Not found: /Users/gleonardo/Desktop/DeepLearning/Lesson3/Notebooks/dickens_experiments/exp_Dec12_22-18-56/ckpts; No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-5ae0e16e9346>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Load Model at wanted epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m model.load_weights(os.path.join(\n\u001b[0;32m----> 7\u001b[0;31m     cwd, 'dickens_experiments', 'exp_Dec12_22-18-56', 'ckpts', 'cp_100.ckpt'))\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Get random slice from text of length 2*seq_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name)\u001b[0m\n\u001b[1;32m    179\u001b[0m         raise ValueError('Load weights is not yet supported with TPUStrategy '\n\u001b[1;32m    180\u001b[0m                          'with steps_per_run greater than 1.')\n\u001b[0;32m--> 181\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mtrackable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_automatic_dependency_tracking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name)\u001b[0m\n\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1143\u001b[0;31m         \u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNewCheckpointReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1144\u001b[0m         \u001b[0msave_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'tf'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0merrors_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLossError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mNewCheckpointReader\u001b[0;34m(filepattern)\u001b[0m\n\u001b[1;32m    871\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mNewCheckpointReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mCheckpointReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[0mNewCheckpointReader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tf_api_names_v1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'train.NewCheckpointReader'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mthis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_pywrap_tensorflow_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_CheckpointReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to get matching files on /Users/gleonardo/Desktop/DeepLearning/Lesson3/Notebooks/dickens_experiments/exp_Dec12_22-18-56/ckpts/cp_100.ckpt: Not found: /Users/gleonardo/Desktop/DeepLearning/Lesson3/Notebooks/dickens_experiments/exp_Dec12_22-18-56/ckpts; No such file or directory"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "# Load Model at wanted epoch\n",
    "model.load_weights(os.path.join(\n",
    "    cwd, 'dickens_experiments', 'exp_Dec12_22-18-56', 'ckpts', 'cp_100.ckpt'))\n",
    "\n",
    "# Get random slice from text of length 2*seq_length\n",
    "start_idx = np.random.randint(0, full_text_length - seq_length)\n",
    "\n",
    "text_slice = full_text[start_idx:start_idx+2*seq_length]\n",
    "\n",
    "seed_sentence = text_slice[:seq_length]\n",
    "\n",
    "in_onehot = np.zeros([1, seq_length, len(vocabulary)])\n",
    "for t_idx, c in enumerate(seed_sentence):\n",
    "    in_onehot[:, t_idx, ctoi[c]] = 1.\n",
    "\n",
    "print('\\n----- Seed sequence:')\n",
    "print(seed_sentence)\n",
    "\n",
    "next_char = text_slice[seq_length-1]\n",
    "\n",
    "next_chars = []\n",
    "next_chars.append(next_char)\n",
    "probs = []\n",
    "\n",
    "for i in range(seq_length):\n",
    "\n",
    "    preds = model.predict(in_onehot, verbose=0)[0]\n",
    "    \n",
    "    ordered_preds = np.argsort(preds)[::-1]\n",
    "   \n",
    "    probs.append([itoc[ordered_preds[0]], itoc[ordered_preds[1]], \n",
    "          itoc[ordered_preds[2]], itoc[ordered_preds[3]], \n",
    "          itoc[ordered_preds[4]]])\n",
    "          \n",
    "    next_char = text_slice[seq_length+i]\n",
    "    next_chars.append(next_char)\n",
    "    next_char_id = ctoi[next_char]\n",
    "\n",
    "    next_char_onehot = np.zeros([1, 1, len(vocabulary)])\n",
    "    next_char_onehot[:, :, next_char_id] = 1.\n",
    "\n",
    "    in_onehot = np.concatenate([in_onehot, next_char_onehot], axis=1)\n",
    "    in_onehot = in_onehot[:, 1:, :]\n",
    "    \n",
    "plt.figure(figsize=(20,2))\n",
    "clust_data = np.array(probs).T\n",
    "collabel=next_chars\n",
    "table = plt.table(cellText=clust_data,colLabels=collabel, loc='center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize LSTM hidden neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Unsuccessful TensorSliceReader constructor: Failed to get matching files on /Users/gleonardo/Desktop/DeepLearning/Lesson3/Notebooks/dickens_experiments/exp_Dec12_22-18-56/ckpts/cp_100.ckpt: Not found: /Users/gleonardo/Desktop/DeepLearning/Lesson3/Notebooks/dickens_experiments/exp_Dec12_22-18-56/ckpts; No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-8a3af0b5cf69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load Model at wanted epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m model.load_weights(os.path.join(\n\u001b[0;32m----> 5\u001b[0;31m    cwd, 'dickens_experiments', 'exp_Dec12_22-18-56', 'ckpts', 'cp_100.ckpt'))\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Create a new model to get neurons activations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name)\u001b[0m\n\u001b[1;32m    179\u001b[0m         raise ValueError('Load weights is not yet supported with TPUStrategy '\n\u001b[1;32m    180\u001b[0m                          'with steps_per_run greater than 1.')\n\u001b[0;32m--> 181\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mtrackable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_automatic_dependency_tracking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name)\u001b[0m\n\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1143\u001b[0;31m         \u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNewCheckpointReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1144\u001b[0m         \u001b[0msave_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'tf'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0merrors_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLossError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mNewCheckpointReader\u001b[0;34m(filepattern)\u001b[0m\n\u001b[1;32m    871\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mNewCheckpointReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mCheckpointReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[0mNewCheckpointReader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tf_api_names_v1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'train.NewCheckpointReader'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mthis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_pywrap_tensorflow_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_CheckpointReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to get matching files on /Users/gleonardo/Desktop/DeepLearning/Lesson3/Notebooks/dickens_experiments/exp_Dec12_22-18-56/ckpts/cp_100.ckpt: Not found: /Users/gleonardo/Desktop/DeepLearning/Lesson3/Notebooks/dickens_experiments/exp_Dec12_22-18-56/ckpts; No such file or directory"
     ]
    }
   ],
   "source": [
    "from matplotlib import cm\n",
    "\n",
    "# Load Model at wanted epoch\n",
    "model.load_weights(os.path.join(\n",
    "   cwd, 'dickens_experiments', 'exp_Dec12_22-18-56', 'ckpts', 'cp_100.ckpt'))\n",
    "\n",
    "# Create a new model to get neurons activations\n",
    "model_in = model.input \n",
    "model_out = model.layers[1].output # Which recurrent layer (in this case last one)\n",
    "new_model = tf.keras.Model(model_in, model_out)\n",
    "\n",
    "# Get random slice from text of length 2*seq_length\n",
    "start_idx = np.random.randint(0, full_text_length - seq_length)\n",
    "\n",
    "text_slice = full_text[start_idx:start_idx+2*seq_length]\n",
    "\n",
    "seed_sentence = text_slice[:seq_length]\n",
    "\n",
    "in_onehot = np.zeros([1, seq_length, len(vocabulary)])\n",
    "for t_idx, c in enumerate(seed_sentence):\n",
    "    in_onehot[:, t_idx, ctoi[c]] = 1.\n",
    "\n",
    "print('\\n----- Seed sequence:')\n",
    "print(seed_sentence)\n",
    "\n",
    "next_char = text_slice[seq_length-1]\n",
    "\n",
    "next_chars = []\n",
    "next_chars.append(next_char)\n",
    "neuron_values = []\n",
    "\n",
    "which_neuron = 8 # which neuron we want to inspect\n",
    "\n",
    "for i in range(seq_length):\n",
    "\n",
    "    lstm_states = new_model.predict(in_onehot, verbose=0)[0]\n",
    "    lstm_final_state = lstm_states\n",
    "    lstm_neuron = lstm_final_state[which_neuron]\n",
    "    neuron_values.append(lstm_neuron)\n",
    "\n",
    "    next_char = text_slice[seq_length+i]\n",
    "    next_chars.append(next_char)\n",
    "    next_char_id = ctoi[next_char]\n",
    "\n",
    "    next_char_onehot = np.zeros([1, 1, len(vocabulary)])\n",
    "    next_char_onehot[:, :, next_char_id] = 1.\n",
    "\n",
    "    in_onehot = np.concatenate([in_onehot, next_char_onehot], axis=1)\n",
    "    in_onehot = in_onehot[:, 1:, :]\n",
    "    \n",
    "plt.figure(figsize=(20,2))\n",
    "clust_data = np.expand_dims(np.array(neuron_values), -1).T\n",
    "collabel=next_chars\n",
    "norm = plt.Normalize(min(neuron_values)-1, max(neuron_values)+1)\n",
    "colours = plt.cm.hot(norm(neuron_values))\n",
    "viridis = cm.get_cmap('PuBu_r', 100)\n",
    "table = plt.table(cellText=np.expand_dims(np.array(collabel[:-1]), -1).T,\n",
    "                  cellColours=np.expand_dims(colours, 0), # np.expand_dims(neuron_values, -1).T,\n",
    "                  # colLabels=collabel, \n",
    "                  # colColours=['b','g','r'], \n",
    "                  loc='center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
